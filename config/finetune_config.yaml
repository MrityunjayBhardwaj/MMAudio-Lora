defaults:
  - base_config
  - override hydra/job_logging: custom-simplest
  - _self_

# Override experiment settings
exp_id: finetune_custom
debug: False

# Model selection - use small model for memory efficiency
model: small_16k

# Fine-tuning specific flags
custom_finetune: True
mini_train: False
example_train: False

# Memory-optimized training settings
batch_size: 4  # Small batch size per GPU
gradient_accumulation_steps: 8  # Effective batch size = 32
num_workers: 8  # Reduced workers for memory
pin_memory: True

# Learning rate for fine-tuning (much lower than training)
learning_rate: 1e-5
warmup_steps: 100
max_steps: 5000  # ~7 epochs with 1500 clips
weight_decay: 0.01

# Mixed precision and memory optimizations
amp: True
compile: False  # Disable for stability during fine-tuning
gradient_checkpointing: True
freeze_early_layers: True
freeze_ratio: 0.5  # Freeze 50% of transformer layers

# Regularization to prevent overfitting
dropout_rate: 0.2
label_smoothing: 0.1

# Data augmentation
augmentation:
  enabled: True
  time_masking: True
  freq_masking: True
  mixup_alpha: 0.2
  noise_injection: 0.01

# Evaluation and checkpointing
eval_every: 500  # Evaluate every 500 steps
save_every: 500
early_stopping:
  enabled: True
  patience: 3  # Stop if no improvement for 3 evaluations
  min_delta: 0.001

# Loss configuration
loss_weights:
  reconstruction: 1.0
  perceptual: 0.1
  temporal_consistency: 0.05

# EMA configuration for fine-tuning
ema:
  enable: True
  sigma_rels: [0.05, 0.1]
  update_every: 1
  checkpoint_every: 1000
  default_output_sigma: 0.05

# Sampling configuration for evaluation
sampling:
  num_steps: 50  # More steps for better eval quality
  cfg_strength: 3.0  # Lower than default for more diversity
  temperature: 0.9

# Validation split
validation:
  enabled: True
  split_ratio: 0.1  # Use 10% of data for validation
  random_seed: 42

# Logging
logging:
  log_every: 50
  log_gradients: True
  log_weights: False
  tensorboard: True
  
# Hardware optimization for A100
hardware:
  enable_tf32: True
  enable_flash_attention: True
  prefetch_factor: 2
  persistent_workers: True